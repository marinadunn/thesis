{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6fe0f99",
   "metadata": {},
   "source": [
    "Created by Marina Dunn, Spring 2022\n",
    "\n",
    "**Resources used:**\n",
    "\n",
    "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n",
    "\n",
    "https://github.com/deepskies/deepmerge-public/blob/master/DeepMerge-noisy.ipynb\n",
    "\n",
    "https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\n",
    "\n",
    "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
    "\n",
    "https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-softmax-crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f911effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system level\n",
    "import os\n",
    "import codecs\n",
    "import sys\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import interp\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import __future__\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# machine learning\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Flatten, Dense\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, brier_score_loss\n",
    "\n",
    "# hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.patches as mpatches\n",
    "# Graphics in retina format are more sharp and legible\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5215f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25849359",
   "metadata": {},
   "source": [
    "### Load and define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0a073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load year 1 train, test, validation image files\n",
    "X_train_1 = np.load('images_Y1_train.npy')\n",
    "X_test_1 = np.load('images_Y1_test.npy')\n",
    "X_valid_1 = np.load('images_Y1_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa5188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load year 1, label small subset test files\n",
    "X_test_1_sub = np.load('images_Y1_test_150.npy')\n",
    "Y_test_sub = np.load('labels_test_150.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121691e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image label files\n",
    "Y_train = np.load('labels_train.npy')\n",
    "Y_test = np.load('labels_test.npy')\n",
    "Y_valid = np.load('labels_valid.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd31d1c",
   "metadata": {},
   "source": [
    "**Choosing batch size, epoch:**\n",
    "\n",
    "batch size: larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster. \n",
    "epoch: models improve with more epochs of training, to a point; will start to plateau in accuracy as they converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa95586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial training variables for model 1\n",
    "# 70:10:20 for training:valid:test\n",
    "NUM_TRAIN = 23487 # num train images\n",
    "NUM_TEST = 6715 # num test images\n",
    "NUM_VALIDATION = 3355 # num validation images\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf53ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "random.seed(5)\n",
    "idx = np.random.choice(len(X_train_1), size=len(X_train_1), replace=False)\n",
    "X_train_1 = X_train_1[idx]\n",
    "idx = np.random.choice(len(X_test_1), size=len(X_test_1), replace=False)\n",
    "X_test_1 = X_test_1[idx]\n",
    "idx = np.random.choice(len(X_valid_1), size=len(X_valid_1), replace=False)\n",
    "X_valid_1 = X_valid_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00ac5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(Y_train), size=len(Y_train), replace=False)\n",
    "Y_train = Y_train[idx]\n",
    "idx = np.random.choice(len(Y_test), size=len(Y_test), replace=False)\n",
    "Y_test = Y_test[idx]\n",
    "idx = np.random.choice(len(Y_valid), size=len(Y_valid), replace=False)\n",
    "Y_valid = Y_valid[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11cc5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data types as floats\n",
    "X_train_1 = X_train_1.astype('float32')\n",
    "X_test_1 = X_test_1.astype('float32')\n",
    "X_valid_1 = X_valid_1.astype('float32')\n",
    "X_test_1_sub = X_test_1_sub.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd18931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33557\n"
     ]
    }
   ],
   "source": [
    "# check data sizes\n",
    "NUM_TOTAL = NUM_TRAIN + NUM_TEST + NUM_VALIDATION\n",
    "print(NUM_TOTAL)\n",
    "assert NUM_TOTAL == len(X_train_1) + len(X_test_1) + len(X_valid_1), \"total training, test, validation samples not equal to total samples - exiting\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e6c93",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f6e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_df = pd.DataFrame(Y_train)\n",
    "# Y_test_df = pd.DataFrame(Y_test)\n",
    "# Y_valid_df = pd.DataFrame(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec68bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train_df.info()\n",
    "#Y_test_df.info()\n",
    "#Y_valid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3285360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train_df.value_counts()\n",
    "#Y_test_df.value_counts()\n",
    "#Y_valid_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "999e1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train_df.describe()\n",
    "#Y_test_df.describe()\n",
    "#Y_valid_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f73e6",
   "metadata": {},
   "source": [
    "Each image represents a series of pixels in a grid-like format with a value of brightness for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e41c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Training Set:  (23487, 3, 100, 100) (23487, 3)\n",
      "Test Set:  (6715, 3, 100, 100) (6715, 3)\n",
      "Validation Set:  (3355, 3, 100, 100) (3355, 3)\n"
     ]
    }
   ],
   "source": [
    "print( \"Data dimensions: \")\n",
    "# training on year 1\n",
    "print( \"Training Set: \", np.shape(X_train_1), np.shape(Y_train)) # same for year 10\n",
    "print( \"Test Set: \", np.shape(X_test_1), np.shape(Y_test)) # same for year 10\n",
    "print( \"Validation Set: \", np.shape(X_valid_1), np.shape(Y_valid)) # same for year 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "264f40ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Test Set:  (150, 3, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Subset Test Set: \", np.shape(X_test_1_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fc583",
   "metadata": {},
   "source": [
    "```\n",
    "Multi-Class Classification\n",
    "3 columns: spiral (or '0'), elliptical ('1'), or a galaxy merger ('2’) - (num_classes – 1)\n",
    "No null values, min:0, max:1\n",
    "\n",
    "(year 1)\n",
    "Training set details:\n",
    "• 23487 total images\n",
    "• 10017 Spiral galaxies (mean: 0.426491, std: 0.494577)\n",
    "• 5705 Elliptical galaxies (mean: 0.242900, std: 0.428844)\n",
    "• 7765 Merger galaxies (mean: 0.330608, std: 0.470442)\n",
    "• Memory usage: 550.6 KB\n",
    "\n",
    "Test set details:\n",
    "• 6715 total images\n",
    "• 2863 Spiral galaxies (mean: 0.426359, std: 0.494584)\n",
    "• 1631 Elliptical galaxies (mean: 0.242889, std: 0.428861)\n",
    "• 2221 Merger galaxies (mean: 0.330752, std: 0.470519)\n",
    "• Memory usage: 157.5 KB\n",
    "\n",
    "Validation set details:\n",
    "• 3355 total images\n",
    "• 1432 Spiral galaxies (mean: 0.426826, std: 0.494690)\n",
    "• 815 Elliptical galaxies (mean: 0.242921, std: 0.428912)\n",
    "• 1108 Merger galaxies (mean: 0.330253, std: 0.470374)\n",
    "• Memory usage: 78.8 KB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a4bb8",
   "metadata": {},
   "source": [
    "# Build deterministic CNN model:\n",
    "First build a standard deterministic CNN classifier model as a baseline model. CNN/ConvNet typically uses ~3 convolution layers: a convolutional layer, a pooling layer, and a fully connected layer. The layer setup is such that simpler patterns are recognized first, then increasingly complex patterns thereafter.\n",
    "\n",
    "Perform series of convolution + pooling operations, then a number of fully connected layers. \n",
    "\n",
    "**Convolutional Layer:** Contains most of the network's main computational load. Performs dot product between kernel (matrix 1) and restricted portion of the receptive field (matrix 2). Forward pass. Creates an activation/feature map, or 2D representation of the image.\n",
    "\n",
    "**Pooling Layer:** provides summary statistic of nearby outputs at various network output locations. Pooling operation performed individually on every slice of activation map. Max pooling is most popular.\n",
    "\n",
    "**Fully Connected Layer:** helps map the representation b/w input and the output. Matrix multiplication, followed by a bias effect. Helps to learn non-linear combinations of features.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1) Train\n",
    "\n",
    "2) Validate on a holdout set generated from the original training data\n",
    "\n",
    "3) Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462147a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbddad",
   "metadata": {},
   "source": [
    "#### Defining CNN model architecture:\n",
    "\n",
    "##### Activation functions:\n",
    "\n",
    "**-Sigmoid:** σ(κ) = 1/(1+e¯κ), takes real-value number & “squashes” into a range 0-1\n",
    "\n",
    "**-ReLU/'relu':** Rectified Linear Unit, calculates ƒ(κ)=max (0,κ), or the activation is simply threshold at 0. More reliable + faster than sigmoid, tanh. More non-linearity gives more power to the model\n",
    "\n",
    "**-Softmax/Normalized Exponential:** converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
    "\n",
    "##### Keras layers:\n",
    "\n",
    "**Convolution2D():** 2D convolution layer (e.g. spatial convolution over images). \n",
    "\n",
    "-args: activation (activation function to use), \n",
    "\n",
    "**MaxPooling2D():** Max pooling operation for 2D spatial data\n",
    "\n",
    "-args: pool_size (window size over which to take the maximum), strides (Specifies how far the pooling window moves for each pooling step; sliding size of the kernel), padding (''valid\"- no padding, or ''same\"- padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input), data_format (ordering of the dimensions in the inputs)\n",
    "\n",
    "data_format='channels_first': \n",
    "\n",
    "input: 4D tensor with shape (batch_size, channels, rows, cols)\n",
    "output: 4D tensor with shape (batch_size, channels, pooled_rows, pooled_cols).\n",
    "\n",
    "**Dense():** regular densely-connected neural network layer (describes a layer of neurons)\n",
    "\n",
    "-args: activation (activation function to use), kernel_regularizer (regularizer function applied to the kernel weights matrix)\n",
    "\n",
    "##### Regularization\n",
    "**Dropout:** Most popular regularization technique, prevents overfitting. At each iteration, neuron temporarily “dropped” with probability 'p'. This neuron is resampled with 'p' at every training step, so will be active at next step. Typically ~0.5 or 50%. Can be applied to input or hidden layer nodes, but not output nodes. \n",
    "\n",
    "**Batch Normalization:** explicitly forces the activations throughout a network to take on a Gaussian distribution at the beginning of training, applying a transformation that maintains mean output close to 0 and output standard deviation close to 1. BatchNorm layer inserted immediately after fully connected layers, or convolutional layers, and before non-linearities. Significantly more robust to bad initialization.\n",
    "\n",
    "##### Keras Early stopping\n",
    "**Early stopping:** Early stopping call back function can be used to monitor either accuracy or loss, stopping when there's either a loss increment, or accuracy decrement. Should monitor the thing that is more sensitive, which is usually loss function- will start seeing signs of overfitting in loss earlier. \n",
    "\n",
    "##### Keras Optimizers:  \n",
    "**Adam:** implements the Adam algorithm; learning rate of 0.001. Stochastic gradient descent method based on adaptive estimation of 1st-order and 2nd-order moments. Very performant.\n",
    "\n",
    "##### Loss function: \n",
    "**Categorical Cross-entropy:** Computes the cross-entropy loss between the labels and predictions. Used when adjusting model weights during training. Categorical cross-entropy is used with 'softmax' activation for multi-class classification problem, i.e. when true label values are one-hot encoded for 3-class classification problem.\n",
    "\n",
    "**Sparse Categorical Cross-entropy:** Same as above, but does not require one-hot encoding. Used when there are many labels.\n",
    "\n",
    "##### Additional techniques to potentially look at:\n",
    "**Cross validation:** (i.e. k-fold), partition the original training data set into k equal subsets, each called a fold (named f1, f2, …, fk, for i = 1 to i = k). Keep the fold fi as the Validation set, and keep all the remaining k-1 folds in the Cross validation training set. Train model using the cross validation training set; calculate accuracy by validating predicted results against the validation set. Estimate accuracy of model by averaging the accuracies derived in all the k cases of cross validation. Helps reduce overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09808d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spiral', 'Elliptical', 'Merger']\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Spiral', 'Elliptical', 'Merger']\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1390e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/marinadunn/Desktop/Grad_School/Research/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "f_model = os.path.join(base_dir, \"model.json\") # model data (architecture)\n",
    "f_history = os.path.join(base_dir, \"history.json\") # training history \n",
    "f_weights = os.path.join(base_dir, \"weights.h5\") # model data (final weights)\n",
    "f_best_weights = os.path.join(base_dir, \"best_weights.h5\") # model data (best weights, give highest validation accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a94405",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4304fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c8f9034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 18:45:08.568928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-28 18:45:08.569388: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Defining model layers, build with the Functional API\n",
    "\n",
    "#(channels, rows, cols)\n",
    "input_shape = (3, 100, 100)\n",
    "\n",
    "# Constraints for layer 1\n",
    "# input: tensors of shape (color_channels, image_height, image_width), ignoring batch size\n",
    "x = Input(shape=input_shape)\n",
    "#For CNN: Convolution typically uses 3x3 windows, stride 1 & with padding. Will try 5x5 window first to attempt tp catch more features, but try multiple\n",
    "c0 = Conv2D(8, (5, 5), activation='relu', strides=(1, 1), padding='same', data_format='channels_first', name='conv_1')(x)\n",
    "# normalize input for first layer & also apply to each of the hidden layers\n",
    "b0 = BatchNormalization()(c0)\n",
    "#For CNN: pooling typically uses 2x2 windows, stride 2 & no padding\n",
    "d0 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_first', name='maxpool_1')(b0)\n",
    "e0 = Dropout(0.5)(d0)\n",
    "\n",
    "# Constraints for layer 2\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same', data_format='channels_first', name='conv_2')(e0)\n",
    "b1 = BatchNormalization()(c1)\n",
    "d1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_first', name='maxpool_2')(b1)\n",
    "e1 = Dropout(0.5)(d1)\n",
    "\n",
    "# Constraints for layer 3\n",
    "c2 = Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same', data_format='channels_first', name='conv_3')(e1)\n",
    "b2 = BatchNormalization()(c2)\n",
    "d2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_first', name='maxpool_3')(b2)\n",
    "e2 = Dropout(0.5)(d2)\n",
    "\n",
    "#flatten (or unroll) the 3D output to 1D\n",
    "f = Flatten()(e2)\n",
    "\n",
    "# hidden layer\n",
    "# Dense layer: takes 1D vector as input, current output is a 3D tensor; start simple, then change unit to try to increase performance\n",
    "z0 = Dense(512, activation='relu', kernel_regularizer=l2(0.0001), name='dense_1')(f)\n",
    "z1 = Dense(256, activation='relu', kernel_regularizer=l2(0.0001), name='dense_2')(z0)\n",
    "# use L2 regulization: take sum of all params squared & add it w/ square difference of actual output & predictions\n",
    "# weights not sparse, will get much better accuracy than L1\n",
    "z2 = Dense(64, activation='relu', kernel_regularizer=l2(0.0001), name='dense_3')(z1)\n",
    "z3 = Dense(32, activation='relu', kernel_regularizer=l2(0.0001), name='dense_4')(z2)\n",
    "# output layer must create 3 output values, one for each class\n",
    "# use “softmax” activation function to ensure output values are in range 0-1 & may be used as predicted probabilities\n",
    "y = Dense(NUM_CLASSES, activation='softmax', name='output')(z3)\n",
    "\n",
    "model1 = Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89459ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 100, 100)]     0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, 8, 100, 100)       608       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8, 100, 100)      400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " maxpool_1 (MaxPooling2D)    (None, 8, 50, 50)         0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 50, 50)         0         \n",
      "                                                                 \n",
      " conv_2 (Conv2D)             (None, 16, 50, 50)        1168      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 50, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_2 (MaxPooling2D)    (None, 16, 25, 25)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 25, 25)        0         \n",
      "                                                                 \n",
      " conv_3 (Conv2D)             (None, 32, 25, 25)        4640      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 25, 25)       100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_3 (MaxPooling2D)    (None, 32, 12, 12)        0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 12, 12)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               2359808   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,516,879\n",
      "Trainable params: 2,516,529\n",
      "Non-trainable params: 350\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile Model\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "# Multi-Class Cross-Entropy Loss\n",
    "loss = 'categorical_crossentropy' \n",
    "model1.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c851560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "monitor: value to be monitored by the function: validation loss or validation accuracy\n",
    "mode: mode in which change in the quantity monitored should be observed: ‘min’, ‘max’, or ‘auto’. \n",
    "For loss, value is ‘min’. For accuracy, value is ‘max’. ‘auto’: monitors with auto suitable mode\n",
    "patience: number of epochs for the training to be continued after the first halt for any improvement in the model\n",
    "verbose: integer value- 0, 1 or 2, selects the way in which progress is displayed while training\n",
    "Verbose = 1: A bar depicting the progress of training is displayed\n",
    "\"\"\"\n",
    "\n",
    "es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
    "# Save best weights in order to maximize validation accuracy, only saves when the model is considered the \"best\"\n",
    "     ModelCheckpoint(filepath=f_best_weights, monitor='val_acc', mode='max', verbose=1, save_best_only=True)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3ddebdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "#plot_model(model1, \"model1_diagram.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf3cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "history = model1.fit(\n",
    "                    X_train_1, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=NUM_EPOCH,\n",
    "                    validation_data=(X_valid_1, Y_valid),                \n",
    "                    shuffle=shuffle,\n",
    "                    verbose=True,\n",
    "                    callbacks=es\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6311a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "model1.save_weights(f_weights, overwrite=True)\n",
    "open(f_model, 'w').write(model1.to_json())\n",
    "\n",
    "#Saving history for future use\n",
    "with open(f_history, 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e7376f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 21:03:38.054615: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-10 21:03:38.476137: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-10 21:03:38.636154: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#loading history\n",
    "path = f_history\n",
    "if os.path.exists(path): # reload history if it exists\n",
    "        with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "             n = json.loads(f.read())\n",
    "                \n",
    "#loading model\n",
    "json_file = open(f_model, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "# load best weights into new model\n",
    "loaded_model.load_weights(f_weights)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6482c878",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [112]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m      3\u001b[0m score \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mevaluate(X_test_1, Y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (loaded_model\u001b[38;5;241m.\u001b[39mmetrics_names[\u001b[38;5;241m1\u001b[39m], score[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loaded_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "score = loaded_model.evaluate(X_test_1, Y_test, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "697352fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 35ms/step - loss: 0.8892 - accuracy: 0.8333\n",
      "accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test smaller subset\n",
    "score = loaded_model.evaluate(X_test_1_sub, Y_test_sub, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edc620a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize training results (Loss)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize training results (Loss)\n",
    "\n",
    "# Accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = list(range(len(loss)))\n",
    "\n",
    "#plot\n",
    "figsize=(6,4)\n",
    "fig, axis1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n",
    "plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"Validation Accuracy\")\n",
    "\n",
    "plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n",
    "plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plots = plot1_loss + plot1_val_loss\n",
    "labs = [l.get_label() for l in plots]\n",
    "\n",
    "axis1.set_xlabel('Epoch')\n",
    "axis1.set_ylabel('Loss/Accuracy')\n",
    "plt.title(\"Model 1 Loss/Accuracy History (Year 1 Images)\")\n",
    "plt.tight_layout()\n",
    "axis1.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c5976",
   "metadata": {},
   "source": [
    "Training loss continues to go down, but validation loss starts to go up at epoch 10. Starts overfitting at epoch 10. Look for additional normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1292ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for test set\n",
    "# predict\n",
    "prob = loaded_model.predict(X_test_1)\n",
    "pred = np.round(prob, 0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0760976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '      Spiral       0.86      0.88      0.87      2863\\n'\n",
      " '  Elliptical       0.80      0.73      0.76      1631\\n'\n",
      " '      Merger       0.78      0.78      0.78      2221\\n'\n",
      " '\\n'\n",
      " '   micro avg       0.82      0.81      0.82      6715\\n'\n",
      " '   macro avg       0.82      0.80      0.81      6715\\n'\n",
      " 'weighted avg       0.82      0.81      0.82      6715\\n'\n",
      " ' samples avg       0.81      0.81      0.81      6715\\n')\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "from pprint import pprint\n",
    "classification_metrics = classification_report(Y_test, pred, target_names=class_names)\n",
    "pprint(classification_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6dae296",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_test_labels = pd.DataFrame(Y_test).idxmax(axis=1)\n",
    "categorical_preds = pd.DataFrame(pred).idxmax(axis=1)\n",
    "confusion_matrix = confusion_matrix(categorical_test_labels, categorical_preds)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "   normalize=False,\n",
    "   title='Confusion matrix',\n",
    "   cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b06b1b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 293,
       "width": 346
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22e543f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.811914\n",
      "Precision (samples): 0.811914\n",
      "Recall (samples): 0.811914\n",
      "F1 score (samples): 0.811914\n",
      "Precision (micro): 0.821951\n",
      "Recall (micro): 0.811914\n",
      "F1 score (micro): 0.816901\n",
      "Precision (weighted): 0.821131\n",
      "Recall (weighted): 0.811914\n",
      "F1 score (weighted): 0.816085\n"
     ]
    }
   ],
   "source": [
    "labels = [0, 1, 2]\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='samples')\n",
    "print('Precision (samples): %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, pred, labels=labels, average='samples')\n",
    "print('Recall (samples): %f' % recall)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='micro')\n",
    "print('Precision (micro): %f' % precision)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='weighted')\n",
    "print('Precision (weighted): %f' % precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937053d",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "Changes:\n",
    "\n",
    "Increase dropout rates after first\n",
    "\n",
    "Lower batch size\n",
    "\n",
    "Increased the number of Conv2D layers by 2\n",
    "\n",
    "Add padding to Maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a6d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
    "# Save best weights in order to maximize validation accuracy, only saves when the model is considered the \"best\"\n",
    "     ModelCheckpoint(filepath='model/best_weights', monitor='val_acc', mode='max', verbose=1, save_best_only=True)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "472a7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3, 100, 100)]     0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, 8, 100, 100)       608       \n",
      "                                                                 \n",
      " conv_2 (Conv2D)             (None, 8, 100, 100)       584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 100, 100)      400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_1 (MaxPooling2D)    (None, 8, 50, 50)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 50, 50)         0         \n",
      "                                                                 \n",
      " conv_3 (Conv2D)             (None, 16, 50, 50)        1168      \n",
      "                                                                 \n",
      " conv_4 (Conv2D)             (None, 16, 50, 50)        2320      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 50, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_2 (MaxPooling2D)    (None, 16, 25, 25)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16, 25, 25)        0         \n",
      "                                                                 \n",
      " conv_5 (Conv2D)             (None, 32, 25, 25)        4640      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 25, 25)       100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_3 (MaxPooling2D)    (None, 32, 13, 13)        0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32, 13, 13)        0         \n",
      "                                                                 \n",
      " conv_6 (Conv2D)             (None, 64, 13, 13)        18496     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64, 13, 13)       52        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " maxpool_4 (MaxPooling2D)    (None, 64, 7, 7)          0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64, 7, 7)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                200768    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 231,515\n",
      "Trainable params: 231,139\n",
      "Non-trainable params: 376\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (3, 100, 100)\n",
    "\n",
    "# Constraints for block 1\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same', data_format='channels_first', name='conv_1')(inputs)\n",
    "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_1')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "# Constraints for block 2\n",
    "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_3')(x)\n",
    "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_2')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# Constraints for block 3\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_5')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_3')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# Constraints for block 4\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_6')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_4')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# flatten (or unroll) the 3D output to 1D\n",
    "x = Flatten()(x)\n",
    "\n",
    "# hidden layers\n",
    "x = Dense(64, activation='relu', kernel_regularizer=l2(0.0001), name='dense_1')(x)\n",
    "x = Dense(32, activation='relu', kernel_regularizer=l2(0.0001), name='dense_2')(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model2.compile( \n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b89d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "NUM_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abca1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.1071 - accuracy: 0.4157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 18:48:50.543230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 120s 325ms/step - loss: 1.1071 - accuracy: 0.4157 - val_loss: 1.0906 - val_accuracy: 0.4268\n",
      "Epoch 2/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.4259WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0912 - accuracy: 0.4259 - val_loss: 1.0899 - val_accuracy: 0.4268\n",
      "Epoch 3/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0887 - accuracy: 0.4264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 315ms/step - loss: 1.0887 - accuracy: 0.4264 - val_loss: 1.0901 - val_accuracy: 0.4268\n",
      "Epoch 4/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4262WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 317ms/step - loss: 1.0878 - accuracy: 0.4262 - val_loss: 1.0898 - val_accuracy: 0.4268\n",
      "Epoch 5/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0857 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0857 - accuracy: 0.4265 - val_loss: 1.0883 - val_accuracy: 0.4268\n",
      "Epoch 6/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0841 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 320ms/step - loss: 1.0841 - accuracy: 0.4265 - val_loss: 1.0872 - val_accuracy: 0.4268\n",
      "Epoch 7/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0826 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 118s 321ms/step - loss: 1.0826 - accuracy: 0.4265 - val_loss: 1.0857 - val_accuracy: 0.4268\n",
      "Epoch 8/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0811 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0811 - accuracy: 0.4265 - val_loss: 1.0843 - val_accuracy: 0.4268\n",
      "Epoch 9/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0798 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0798 - accuracy: 0.4265 - val_loss: 1.0819 - val_accuracy: 0.4268\n",
      "Epoch 10/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0785 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 315ms/step - loss: 1.0785 - accuracy: 0.4265 - val_loss: 1.0799 - val_accuracy: 0.4268\n",
      "Epoch 11/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0775 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 320ms/step - loss: 1.0775 - accuracy: 0.4265 - val_loss: 1.0788 - val_accuracy: 0.4268\n",
      "Epoch 12/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 318ms/step - loss: 1.0765 - accuracy: 0.4265 - val_loss: 1.0779 - val_accuracy: 0.4268\n",
      "Epoch 13/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0758 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0758 - accuracy: 0.4265 - val_loss: 1.0767 - val_accuracy: 0.4268\n",
      "Epoch 14/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0752 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 320ms/step - loss: 1.0752 - accuracy: 0.4265 - val_loss: 1.0761 - val_accuracy: 0.4268\n",
      "Epoch 15/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0747 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0747 - accuracy: 0.4265 - val_loss: 1.0755 - val_accuracy: 0.4268\n",
      "Epoch 16/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0822 - accuracy: 0.4255WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0822 - accuracy: 0.4255 - val_loss: 1.0769 - val_accuracy: 0.4268\n",
      "Epoch 17/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 315ms/step - loss: 1.0763 - accuracy: 0.4265 - val_loss: 1.0756 - val_accuracy: 0.4268\n",
      "Epoch 18/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0753 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 115s 313ms/step - loss: 1.0753 - accuracy: 0.4265 - val_loss: 1.0749 - val_accuracy: 0.4268\n",
      "Epoch 19/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0747 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 114s 311ms/step - loss: 1.0747 - accuracy: 0.4265 - val_loss: 1.0744 - val_accuracy: 0.4268\n",
      "Epoch 20/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0744 - accuracy: 0.4265 - val_loss: 1.0741 - val_accuracy: 0.4268\n",
      "Epoch 21/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0741 - accuracy: 0.4265 - val_loss: 1.0738 - val_accuracy: 0.4268\n",
      "Epoch 22/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0739 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0739 - accuracy: 0.4265 - val_loss: 1.0737 - val_accuracy: 0.4268\n",
      "Epoch 23/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 315ms/step - loss: 1.0737 - accuracy: 0.4265 - val_loss: 1.0735 - val_accuracy: 0.4268\n",
      "Epoch 24/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0736 - accuracy: 0.4265 - val_loss: 1.0734 - val_accuracy: 0.4268\n",
      "Epoch 25/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0736 - accuracy: 0.4265 - val_loss: 1.0733 - val_accuracy: 0.4268\n",
      "Epoch 26/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0734 - accuracy: 0.4265 - val_loss: 1.0733 - val_accuracy: 0.4268\n",
      "Epoch 27/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0749 - accuracy: 0.4259WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 115s 314ms/step - loss: 1.0749 - accuracy: 0.4259 - val_loss: 1.0738 - val_accuracy: 0.4268\n",
      "Epoch 28/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0739 - accuracy: 0.4263WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 115s 314ms/step - loss: 1.0739 - accuracy: 0.4263 - val_loss: 1.0734 - val_accuracy: 0.4268\n",
      "Epoch 29/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0735 - accuracy: 0.4265 - val_loss: 1.0733 - val_accuracy: 0.4268\n",
      "Epoch 30/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0734 - accuracy: 0.4265 - val_loss: 1.0732 - val_accuracy: 0.4268\n",
      "Epoch 31/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 115s 314ms/step - loss: 1.0734 - accuracy: 0.4265 - val_loss: 1.0732 - val_accuracy: 0.4268\n",
      "Epoch 32/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 115s 313ms/step - loss: 1.0733 - accuracy: 0.4265 - val_loss: 1.0731 - val_accuracy: 0.4268\n",
      "Epoch 33/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.4264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0733 - accuracy: 0.4264 - val_loss: 1.0731 - val_accuracy: 0.4268\n",
      "Epoch 34/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.4268WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 316ms/step - loss: 1.0732 - accuracy: 0.4268 - val_loss: 1.0736 - val_accuracy: 0.4265\n",
      "Epoch 35/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.4260WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 315ms/step - loss: 1.0744 - accuracy: 0.4260 - val_loss: 1.0734 - val_accuracy: 0.4265\n",
      "Epoch 36/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.4264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0737 - accuracy: 0.4264 - val_loss: 1.0733 - val_accuracy: 0.4268\n",
      "Epoch 37/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.4264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 320ms/step - loss: 1.0735 - accuracy: 0.4264 - val_loss: 1.0732 - val_accuracy: 0.4268\n",
      "Epoch 38/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 118s 322ms/step - loss: 1.0733 - accuracy: 0.4265 - val_loss: 1.0732 - val_accuracy: 0.4268\n",
      "Epoch 39/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.4264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 118s 322ms/step - loss: 1.0733 - accuracy: 0.4264 - val_loss: 1.0731 - val_accuracy: 0.4268\n",
      "Epoch 40/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 117s 319ms/step - loss: 1.0732 - accuracy: 0.4265 - val_loss: 1.0731 - val_accuracy: 0.4268\n",
      "Epoch 41/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.4265WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "367/367 [==============================] - 116s 317ms/step - loss: 1.0733 - accuracy: 0.4265 - val_loss: 1.0731 - val_accuracy: 0.4268\n",
      "Epoch 42/100\n",
      "308/367 [========================>.....] - ETA: 18s - loss: 1.0723 - accuracy: 0.4295"
     ]
    }
   ],
   "source": [
    "history =  model2.fit(X_train_1, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=NUM_EPOCH,\n",
    "                    validation_data=(X_valid_1, Y_valid),\n",
    "                    shuffle=True,\n",
    "                    verbose=True,\n",
    "                    callbacks=es\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, \"model_diagram.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_score = model2.evaluate(X_test_1, Y_test, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], test_score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96978cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test smaller subset\n",
    "test_sub_score = model2.evaluate(X_test_1_sub, Y_test_sub, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], test_sub_score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09edc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for test set\n",
    "# predict\n",
    "prob = model2.predict(X_test_1)\n",
    "pred = np.round(prob, 0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2]\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='samples')\n",
    "print('Precision (samples): %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, pred, labels=labels, average='samples')\n",
    "print('Recall (samples): %f' % recall)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='micro')\n",
    "print('Precision (micro): %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, pred, labels=labels, average='micro')\n",
    "print('Recall (micro): %f' % recall)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, pred, labels=labels, average='weighted')\n",
    "print('Precision (weighted): %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, pred, labels=labels, average='weighted')\n",
    "print('Recall (weighted): %f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2218637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results (Loss)\n",
    "\n",
    "# Accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = list(range(len(loss)))\n",
    "\n",
    "#plot\n",
    "figsize=(6,4)\n",
    "fig, axis1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "plot1_lacc = axis1.plot(epochs, acc, 'navy', label='accuracy')\n",
    "plot1_val_lacc = axis1.plot(epochs, val_acc, 'deepskyblue', label=\"Validation Accuracy\")\n",
    "\n",
    "plot1_loss = axis1.plot(epochs, loss, 'red', label='loss')\n",
    "plot1_val_loss = axis1.plot(epochs, val_loss, 'lightsalmon', label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plots = plot1_loss + plot1_val_loss\n",
    "labs = [l.get_label() for l in plots]\n",
    "\n",
    "axis1.set_xlabel('Epoch')\n",
    "axis1.set_ylabel('Loss/Accuracy')\n",
    "plt.title(\"Model 2 Loss/Accuracy History (Year 1 Images)\")\n",
    "plt.tight_layout()\n",
    "axis1.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "classification_metrics = classification_report(Y_test, pred, target_names=class_names)\n",
    "pprint(classification_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0face2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_test_labels = pd.DataFrame(Y_test).idxmax(axis=1)\n",
    "categorical_preds = pd.DataFrame(pred).idxmax(axis=1)\n",
    "confusion_matrix = confusion_matrix(categorical_test_labels, categorical_preds)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "   normalize=False,\n",
    "   title='Confusion matrix',\n",
    "   cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10915a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80669aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model data if improved accuracy\n",
    "# model2.save_weights(f_weights, overwrite=True)\n",
    "# open(f_model, 'w').write(model2.to_json())\n",
    "\n",
    "# #Saving history for future use\n",
    "# with open(f_history, 'w') as f:\n",
    "#     json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading history\n",
    "# path = base_dir + 'model/history.json'\n",
    "# if os.path.exists(path): # reload history if it exists\n",
    "#         with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "#              n = json.loads(f.read())\n",
    "                \n",
    "# #loading model\n",
    "# json_file = open(f_model, 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "# # load best weights into new model\n",
    "# loaded_model.load_weights(f_best_weights)\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c14c1",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "Changes:\n",
    "\n",
    "Add data augmentation\n",
    "\n",
    "Go back to smaller dropout rate\n",
    "\n",
    "Increase Conv2D filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ce797-091a-4fd0-8fe9-ed7dd35d8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 100, 100)\n",
    "\n",
    "# Constraints for block 1\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(64, kernel_size=(5, 5), activation='relu', padding='same', data_format='channels_first', name='conv_1')(inputs)\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_1')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "# Constraints for block 2\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_3')(x)\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_2')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# Constraints for block 3\n",
    "x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_5')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_3')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# Constraints for block 4\n",
    "x = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first', name='conv_6')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_first', name='maxpool_4')(x)\n",
    "x = Dropout(rate=0.8)(x)\n",
    "\n",
    "# flatten (or unroll) the 3D output to 1D\n",
    "x = Flatten()(x)\n",
    "\n",
    "# hidden layers\n",
    "x = Dense(64, activation='relu', kernel_regularizer=l2(0.0001), name='dense_1')(x)\n",
    "x = Dense(32, activation='relu', kernel_regularizer=l2(0.0001), name='dense_2')(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "model3 = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model3.compile( \n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942725e-42c1-4a91-8e14-c14177bb7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=90,\n",
    "      width_shift_range=0.2,               \n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      vertical_flip = True,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e834cd1-40f2-4b5c-8b5d-3242e2b30d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_test = ImageDataGenerator(\n",
    "      rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63a441-4ab8-4620-b20f-f85b67a2fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train.fit(X_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
    "ModelCheckpoint(filepath='model/best_weights', monitor='val_acc', mode='max', verbose=1, save_best_only=True)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420012bb-640b-4e6f-aba7-ad0d28c08d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "NUM_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada64ed0-31ff-4041-9e38-6a655d2430e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model 3 on batches with data augmentation\n",
    "history =  model3.fit(image_gen_train.flow(X_train_1, Y_train),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=NUM_EPOCH,\n",
    "                    validation_data=image_gen_test.flow(X_valid_1, Y_valid),\n",
    "                    shuffle=True,\n",
    "                    verbose=True,\n",
    "                    callbacks=es\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95048680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = model3.evaluate(X_test_1, Y_test, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test smaller subset\n",
    "score = model3.evaluate(X_test_1_sub, Y_test_sub, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532be68",
   "metadata": {},
   "source": [
    "## Visualizing Feature/Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d1a05",
   "metadata": {},
   "source": [
    "#### Visualize for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the outputs of all layers except the input layer\n",
    "layer_outputs = [layer.output for layer in loaded_model.layers[1:]] \n",
    "# Creates a model that will return these outputs, given the model input\n",
    "activation_model = models.Model(inputs=loaded_model.input, outputs=layer_outputs) \n",
    "\n",
    "# returns the values of the layer activations in original model\n",
    "# Returns a list of Numpy arrays: one array per layer activation\n",
    "activations = activation_model.predict(X_test_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation maps for all 8 filters in the first convolutional layer\n",
    "fig1=plt.figure(figsize=(10,1.5))  \n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i + 1)\n",
    "    layer_activation = activations[0]\n",
    "    plt.imshow(layer_activation[1649, i, :, :], cmap='viridis', aspect='auto')\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "# activation maps for all 16 filters in the second convolutional layer    \n",
    "fig2=plt.figure(figsize=(10,3))  \n",
    "for i in range(16):\n",
    "    plt.subplot(2, 8, i + 1)\n",
    "    layer_activation = activations[4]\n",
    "    plt.imshow(layer_activation[1649, i, :, :], cmap='viridis', aspect='auto')\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "    \n",
    "# activation maps for all 32 filters in the third convolutional layer\n",
    "fig3=plt.figure(figsize=(10,6))  \n",
    "for i in range(32):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    layer_activation = activations[8]\n",
    "    plt.imshow(layer_activation[1649, i, :, :], cmap='viridis', aspect='auto')\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ba588",
   "metadata": {},
   "source": [
    "## Visualizing Example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7493ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Images\n",
    "prob = loaded_model.predict(X_test_1)\n",
    "pred = np.round(prob, 0).astype('int32')\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    if (Y_test[i] == 1 and pred[i,0] == 1): # and other Y_test vs pred combinations TP=(1,1), FP=(0,1), TN=(0,0), FN=(1,0)\n",
    "        print (i)\n",
    "        print(Y_test[i],pred[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ba988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print output probabilities for the chosen TP, FP, TN, FN examples\n",
    "\n",
    "# print(prob[84],prob[1370],prob[2031],prob[3003]) # TP examples\n",
    "# print(prob[560],prob[1228],prob[2878],prob[3026]) # FP examples\n",
    "# print(prob[564],prob[1056],prob[2083],prob[3063]) # TN examples\n",
    "# print(prob[465],prob[1546],prob[2241],prob[3037]) # FN examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04fef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples\n",
    "tp1 = X_test_1[84,1]\n",
    "tp2 = X_test_1[1370,1]\n",
    "tp3 = X_test_1[2031,1]\n",
    "tp4 = X_test_1[3003,1]\n",
    "\n",
    "fp1 = X_test_1[560,1]\n",
    "fp2 = X_test_1[1228,1]\n",
    "fp3 = X_test_1[2878,1]\n",
    "fp4 = X_test_1[3026,1]\n",
    "\n",
    "tn1 = X_test_1[564,1]\n",
    "tn2 = X_test_1[1056,1]\n",
    "tn3 = X_test_1[2083,1]\n",
    "tn4 = X_test_1[3063,1]\n",
    "\n",
    "fn1 = X_test_1[465,1]\n",
    "fn2 = X_test_1[1546,1]\n",
    "fn3 = X_test_1[2241,1]\n",
    "fn4 = X_test_1[3037,1]\n",
    "\n",
    "examples = [tp1, tp2, tp3, tp4, fp1, fp2, fp3, fp4, tn1, tn2, tn3, tn4, fn1, fn2, fn3, fn4]\n",
    "\n",
    "fig1=plt.figure(figsize=(8,8))\n",
    "\n",
    "for i, image in enumerate(examples):\n",
    "    fig1.suptitle('From top row to bottom: TP, FP, TN, FN', fontsize=20)\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image, aspect='auto', cmap='viridis')\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2da41",
   "metadata": {},
   "source": [
    "### Fine Tuning - Transfer Learning with year 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47fe8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load year 10 train, test, validation image files\n",
    "X_train_10 = np.load('images_Y10_train.npy')\n",
    "X_test_10 = np.load('images_Y10_test.npy')\n",
    "X_valid_10 = np.load('images_Y10_valid.npy')\n",
    "X_test_10_sub = np.load('images_Y10_test_150.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10bb56c-8295-4fc2-99c2-7a1e82aff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "random.seed(5)\n",
    "idx = np.random.choice(len(X_train_10), size=len(X_train_10), replace=False)\n",
    "X_train_10 = X_train_10[idx]\n",
    "idx = np.random.choice(len(X_test_10), size=len(X_test_10), replace=False)\n",
    "X_test_10 = X_test_10[idx]\n",
    "idx = np.random.choice(len(X_valid_10), size=len(X_valid_10), replace=False)\n",
    "X_valid_10 = X_valid_10[idx]\n",
    "idx = np.random.choice(len(X_test_10_sub), size=len(X_test_10_sub), replace=False)\n",
    "X_test_10_sub = X_test_10_sub[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ea416-b9af-4b47-baff-0415f7e65142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data types as floats\n",
    "X_train_10 = X_train_10.astype('float32')\n",
    "X_test_10 = X_test_10.astype('float32')\n",
    "X_valid_10 = X_valid_10.astype('float32')\n",
    "X_test_10_sub = X_test_10_sub.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2033f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on year 10 test data\n",
    "score = loaded_model.evaluate(X_test_10, Y_test, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model with\", loaded_model)\n",
    "\n",
    "# Initialize the Pretrained Model\n",
    "feature_extractor = model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input layer\n",
    "input_shape = (3, 100, 100)\n",
    "x = Input(shape=input_shape)\n",
    "\n",
    "# Set the feature extractor layer\n",
    "x = feature_extractor(input_, training=False) # use pre-define weights, not random\n",
    "\n",
    "# Set the pooling layer\n",
    "d0 = GlobalAveragePooling2D()(x)\n",
    "\n",
    "b0 = Dropout(0.5)(c0)\n",
    "\n",
    "# Set the final layer with sigmoid activation function\n",
    "y = Dense(3, activation='softmax', kernel_regularizer=l2(0.0001))(b0)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with year 1 data on year 10 data\n",
    "\n",
    "history = model.fit(\n",
    "                    X_train_1, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=NUM_EPOCH,\n",
    "                    validation_data=(X_valid_1, Y_valid),                \n",
    "                    shuffle=shuffle,\n",
    "                    verbose=True,\n",
    "                    callbacks=es\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1f804",
   "metadata": {},
   "source": [
    "# Probabilistic Bayesian CNN model\n",
    "Add probabilistic layer to end of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "from tensorflow_probability.layers import OneHotCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log-likelihood loss function of each y_true sample given predicted distribution y_pred\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6247d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define probabalistic model architecture\n",
    "data_shape = np.shape(X_1_data)\n",
    "input_shape = (3, 100, 100)\n",
    "dtype = tf.float32\n",
    "\n",
    "bnn_model = Sequential([\n",
    "        Conv2D(kernel_size=(5, 5), filters=8, activation='relu', padding='VALID', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(6, 6)),\n",
    "        Flatten(),\n",
    "        Dense(tfpl.OneHotCategorical.params_size(10)),\n",
    "        tfpl.OneHotCategorical(10, convert_to_tensor_fn=tfd.Distribution.mode)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "bnn_model.compile(loss=nll, optimizer=optimizer, metrics=metrics)\n",
    "probabilistic_model = bnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ac92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the probabalistic model\n",
    "NUM_EPOCH = 100\n",
    "batch_size = 16\n",
    "probabilistic_model.fit(X_train_1, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=NUM_EPOCH,\n",
    "                    validation_data=(X_valid_1, Y_valid),                \n",
    "                    shuffle=shuffle,\n",
    "                    verbose=True,\n",
    "                    callbacks=es\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b09b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "score = probabilistic_model.evaluate(X_test_1, Y_test, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (probabilistic_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test smaller subset\n",
    "score = probabilistic_model.evaluate(X_test_1_sub, Y_test_sub, verbose=True)\n",
    "print(\"%s: %.2f%%\" % (probabilistic_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots of probabilities that the model estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reparameterization Layers\n",
    "def custom_normal_prior(dtype, shape, name, trainable, add_variable_fn):\n",
    "    distribution = tfd.Normal(loc = 0.1 * tf.ones(shape, dtype),\n",
    "                              scale = 1.5 * tf.ones(shape, dtype))\n",
    "    batch_ndims = tf.size(distribution.batch_shape_tensor())\n",
    "    \n",
    "    distribution = tfd.Independent(distribution,\n",
    "                                   reinterpreted_batch_ndims = batch_ndims)\n",
    "    return distribution\n",
    "    \n",
    "def laplace_prior(dtype, shape, name, trainable, add_variable_fn):\n",
    "    distribution = tfd.Laplace(loc = tf.zeros(shape, dtype),\n",
    "                               scale = tf.ones(shape, dtype))\n",
    "    batch_ndims = tf.size(distribution.batch_shape_tensor())\n",
    "    \n",
    "    distribution = tfd.Independent(distribution,\n",
    "                                   reinterpreted_batch_ndims = batch_ndims)\n",
    "    return distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtype = tf.float32\n",
    "distribution = tfd.Normal(loc = tf.zeros(shape, dtype),\n",
    "                            scale = tf.ones(shape, dtype))\n",
    "\n",
    "for i in range(len(shape) + 1):\n",
    "    print('reinterpreted_batch_ndims: %d:' %(i))\n",
    "    independent_dist = tfd.Independent(distribution,\n",
    "                                       reinterpreted_batch_ndims = i)\n",
    "    samples = independent_dist.sample()\n",
    "    print('batch_shape: {}' \n",
    "          ' event_shape: {}' \n",
    "          ' Sample shape: {}'.format(independent_dist._batch_shape(),\n",
    "                                    independent_dist._event_shape(),\n",
    "                                    samples.shape))\n",
    "    print('Samples:', samples.numpy(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divergence_fn: function for the KL-Approximation\n",
    "# Monte Carlo approximation of the KL Divergence\n",
    "def approximate_kl(q, p, q_tensor):\n",
    "    return tf.reduce_mean(q.log_prob(q_tensor) - p.log_prob(q_tensor))\n",
    "\n",
    "total_samples = 60000\n",
    "divergence_fn = lambda q, p, q_tensor : approximate_kl(q, p, q_tensor) / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7564dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_reparameterization_layer(filters, kernel_size, activation):\n",
    "    # For simplicity, we use default prior and posterior.\n",
    "    # In the next parts, we will use custom mixture prior and posteriors.\n",
    "    return tfpl.Convolution2DReparameterization(\n",
    "            filters = filters,\n",
    "            kernel_size = kernel_size,\n",
    "            activation = activation, \n",
    "            padding = 'same',\n",
    "            kernel_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            \n",
    "            bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "            \n",
    "            kernel_divergence_fn = divergence_fn,\n",
    "            bias_divergence_fn = divergence_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_cnn = tf.keras.Sequential([\n",
    "    InputLayer((28, 28, 1)),\n",
    "    \n",
    "    conv_reparameterization_layer(16, 3, 'relu'),\n",
    "    MaxPooling2D(2),\n",
    "    \n",
    "    conv_reparameterization_layer(32, 3, 'relu'),\n",
    "    MaxPooling2D(2),\n",
    "\n",
    "    conv_reparameterization_layer(64, 3, 'relu'),\n",
    "    MaxPooling2D(2),\n",
    "\n",
    "    conv_reparameterization_layer(128, 3, 'relu'),\n",
    "    GlobalMaxPooling2D(),\n",
    "    \n",
    "    tfpl.DenseReparameterization(\n",
    "        units = tfpl.OneHotCategorical.params_size(10), activation = None,\n",
    "        kernel_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "        kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "        \n",
    "        bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "        bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "        \n",
    "        kernel_divergence_fn = divergence_fn,\n",
    "        bias_divergence_fn = divergence_fn),\n",
    "    # Output is a distribution object (OneHotCategorical)\n",
    "    tfpl.OneHotCategorical(10)\n",
    "])\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "bayesian_cnn.compile(loss=nll,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "bayesian_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78260c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at probabilistic model predictions: probabilities the model assigns to each \n",
    "# class instead of its single prediction\n",
    "\n",
    "def analyse_model_prediction(data, true_labels, model, image_num, run_ensemble=False):\n",
    "    if run_ensemble:\n",
    "        ensemble_size = 200\n",
    "    else:\n",
    "        ensemble_size = 1\n",
    "    image = data[image_num]\n",
    "    true_label = true_labels[image_num, 0]\n",
    "    predicted_probabilities = np.empty(shape=(ensemble_size, 10))\n",
    "    for i in range(ensemble_size):\n",
    "        predicted_probabilities[i] = model(image[np.newaxis, :]).mean().numpy()[0]\n",
    "    model_prediction = model(image[np.newaxis, :])\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 2),\n",
    "                                   gridspec_kw={'width_ratios': [2, 4]})\n",
    "    \n",
    "    # Show the image and the true label\n",
    "    ax1.imshow(image[..., 0], cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('True label: {}'.format(str(true_label)))\n",
    "    \n",
    "    # Show a 95% prediction interval of model predicted probabilities\n",
    "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(10)])\n",
    "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(10)])    \n",
    "    bar = ax2.bar(np.arange(10), pct_97p5, color='red')\n",
    "    bar[int(true_label)].set_color('green')\n",
    "    ax2.bar(np.arange(10), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.set_title('Model estimated probabilities')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research_env)",
   "language": "python",
   "name": "research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
